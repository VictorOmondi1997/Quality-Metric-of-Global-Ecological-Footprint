{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing the Quality Metric of Global Ecological Footprint\n",
    "\n",
    "> Managing the Quality Metric of Global Ecological Footprint\n",
    "\n",
    "- author: Victor Omondi\n",
    "- toc: true\n",
    "- comments: true\n",
    "- categories: [classification, machine-learning]\n",
    "- image: images/mqmgef-shield.png"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "![image.png](datasets/images/poster.png \"poster.png\")\n",
    "\n",
    "## Machine Learning: Classification - Managing the Quality Metric of Global Ecological Footprint\n",
    "\n",
    "\n",
    "The dataset used  was obtained from the National Footprint and Biocapacity Accounts. It provides Ecological Footprint per capita data for years 1961-2016 in global hectares (gha). The National Footprint and Biocapacity Accounts (NFAs) measure the ecological resource use and resource capacity of nations from 1961 to 2016. The calculations in the National Footprint and Biocapacity Accounts are primarily based on United Nations data sets.\n",
    "\n",
    "We will use the data to classify and predict the quality metrics (qascore) of the ecological footprint data for the different countries. This data includes total and per capita national biocapacity, the ecological footprint of consumption, the ecological footprint of production and total area in hectares.\n",
    "\n",
    "Data Source: https://data.world/footprint/nfa-2019-edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import (LabelEncoder, \n",
    "                                   MinMaxScaler)\n",
    "from sklearn.model_selection import (cross_val_score, \n",
    "                                     KFold, \n",
    "                                     LeaveOneOut, \n",
    "                                     StratifiedKFold, \n",
    "                                     train_test_split)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix, \n",
    "                             f1_score, \n",
    "                             precision_score, \n",
    "                             recall_score)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will Explore linear classification.\n",
    "\n",
    "In machine learning, classification is a supervised method of segmenting data points into various labels or classes. Unlike regression, the target variable in a classification problem is discrete. Each data point used in training classification models must have a corresponding label in order for the characteristics and patterns in the classes to be learnt appropriately. Classification can either be binary - identifying that a given email is spam or not or, multi-class - classifying a fruit as orange, mango or banana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every year people demand more from nature than it can regenerate. Individuals, communities and government leaders use ecological footprint data to better manage limited resources, reduce economic risk, and improve well-being. The Dataset provides Ecological Footprint per capita data for years 1961-2016 in global hectares (gha). Ecological Footprint is a measure of how much area of biologically productive land and water an individual, population, or activity requires to produce all the resources it consumes and to absorb the waste it generates, using prevailing technology and resource management practices. The Ecological Footprint is measured in global hectares. Since trade is global, an individual or country's Footprint tracks area from all over the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from predicting numeric values, another important supervised machine learning method is classification and it involves predicting classes (either binary or multinomial classes). In this section, we will cover how to measure performances of class prediction, linear classification methods and non-linear/tree-based methods. Weâ€™ll also focus on strategies for applying a successful classification model like interpretability-accuracy trade-off, class and imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The National Footprint and Biocapacity Accounts (NFAs) measure the ecological resource use and resource capacity of nations from 1961 to 2016. The calculations in the National Footprint and Biocapacity Accounts are primarily based on United Nations data sets, including those published by the Food and Agriculture Organization, United Nations Commodity Trade Statistics Database, and the UN Statistics Division, as well as the International Energy Agency. In this project, we will use this data to classify and predict the quality metrics (qascore) of the ecological footprint data for the different countries. This data includes total and per capita national biocapacity, the ecological footprint of consumption, the ecological footprint of production and total area in hectares.\n",
    "\n",
    "Data Source: https://data.world/footprint/nfa-2019-edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classification and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, classification is a supervised method of segmenting data points into various labels or classes. Unlike regression, the target variable in a classification problem is discrete. Each data point used in training classification models must have a corresponding label in order for the characteristics and patterns in the classes to be learnt appropriately. Classification can either be binary - identifying that a given email is spam or not or, multi-class - classifying a fruit as orange, mango or banana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifiers and the importance of class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we define a linear classifier as a binary classifier that separates two classes (positive and negative class) using a linear separator by computing a linear combination of the features and comparing against a set threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Sigmoid, logit and the log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a linear algorithm that can be used for binary or multiclass classification. It is a discriminative classifier that estimates the probability that an instance belongs to a class using an s-shape function curve called the sigmoid function. The predicted values obtained after using a linear equation on the predictors by applying logistic regression can fall in the range of negative infinity to positive infinity. The sigmoid maps these results by shrinking the value to fall between 0 and 1.  We can say that we use the sigmoid function to transform linear regression into logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "sigmoid\\ \\sigma \\ (x) = \\frac{1}{1+e^{-x}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](datasets/images/sigmoid-curve.png \"sigmoid-curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function can be applied to a linear equation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z = \\beta_0 + \\beta_{1}x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to obtain values h between 0 and 1 such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h = \\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-{\\beta_0 + \\beta_{1}x}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classification task with classes A and B, if a threshold is set for 0.5 and the probability of an instance belonging to a class is $p$, we can say that if $p < 0.5$ the instance if of class A while it is of class B is $p > 0.5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as the log of odds, logit is the logarithm of odds ratio where the odds ratio is the probability that an event occurs divided by the probability that the event does not occur. Logit is the inverse of the sigmoid such that it maps values from negative infinity to positive infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log{it}(p) = \\log(\\frac{p}{1 - p})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Recall that in linear regression, we minimized the sum of squared errors SSE; in logistic regression, the log-likelihood is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.199546</td>\n",
       "      <td>0.097188051</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.032351e-01</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>AreaTotHA</td>\n",
       "      <td>483000.000000</td>\n",
       "      <td>687000.000000</td>\n",
       "      <td>334600</td>\n",
       "      <td>127000.000000</td>\n",
       "      <td>100943.000800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.732543e+06</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>BiocapPerCap</td>\n",
       "      <td>0.159804</td>\n",
       "      <td>0.135261</td>\n",
       "      <td>0.084003213</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.262086e-01</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>550176.242700</td>\n",
       "      <td>465677.972200</td>\n",
       "      <td>289207.1078</td>\n",
       "      <td>47311.551720</td>\n",
       "      <td>114982.279300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467355e+06</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>0.387510</td>\n",
       "      <td>0.189462</td>\n",
       "      <td>1.26E-06</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>1.114093</td>\n",
       "      <td>1.728629e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  year  country_code        record      crop_land   grazing_land  \\\n",
       "0  Armenia  1992             1    AreaPerCap       0.140292       0.199546   \n",
       "1  Armenia  1992             1     AreaTotHA  483000.000000  687000.000000   \n",
       "2  Armenia  1992             1  BiocapPerCap       0.159804       0.135261   \n",
       "3  Armenia  1992             1  BiocapTotGHA  550176.242700  465677.972200   \n",
       "4  Armenia  1992             1  EFConsPerCap       0.387510       0.189462   \n",
       "\n",
       "   forest_land  fishing_ground  built_up_land    carbon         total QScore  \n",
       "0  0.097188051        0.036888       0.029320  0.000000  5.032351e-01     3A  \n",
       "1       334600   127000.000000  100943.000800  0.000000  1.732543e+06     3A  \n",
       "2  0.084003213        0.013742       0.033398  0.000000  4.262086e-01     3A  \n",
       "3  289207.1078    47311.551720  114982.279300  0.000000  1.467355e+06     3A  \n",
       "4     1.26E-06        0.004165       0.033398  1.114093  1.728629e+00     3A  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/raw/NFA 2019 public_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72186 entries, 0 to 72185\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         72186 non-null  object \n",
      " 1   year            72186 non-null  int64  \n",
      " 2   country_code    72186 non-null  int64  \n",
      " 3   record          72186 non-null  object \n",
      " 4   crop_land       51714 non-null  float64\n",
      " 5   grazing_land    51714 non-null  float64\n",
      " 6   forest_land     51714 non-null  object \n",
      " 7   fishing_ground  51713 non-null  float64\n",
      " 8   built_up_land   51713 non-null  float64\n",
      " 9   carbon          51713 non-null  float64\n",
      " 10  total           72177 non-null  float64\n",
      " 11  QScore          72185 non-null  object \n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country               0\n",
       "year                  0\n",
       "country_code          0\n",
       "record                0\n",
       "crop_land         20472\n",
       "grazing_land      20472\n",
       "forest_land       20472\n",
       "fishing_ground    20473\n",
       "built_up_land     20473\n",
       "carbon            20473\n",
       "total                 9\n",
       "QScore                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a lot of missing values from `crop_land:carbon` columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51481\n",
       "2A    10576\n",
       "2B    10096\n",
       "1A       16\n",
       "1B       16\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.QScore.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "For simplicity, we will drop the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country           0\n",
       "year              0\n",
       "country_code      0\n",
       "record            0\n",
       "crop_land         0\n",
       "grazing_land      0\n",
       "forest_land       0\n",
       "fishing_ground    0\n",
       "built_up_land     0\n",
       "carbon            0\n",
       "total             0\n",
       "QScore            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51473\n",
       "2A      224\n",
       "1A       16\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.QScore.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An obvious change in our target variable after removing the missing values is that there are only three classes left. From the distribution of the 3 classes, we can see that there is an obvious imbalance between the classes. There are methods that can be applied to handle this imbalance such as oversampling and undersampling.\n",
    "\n",
    "- Oversampling involves increasing the number of instances in the class with fewer instances\n",
    "- Undersampling involves reducing the data points in the class with more instances.\n",
    "For now, we will convert this to a binary classification problem by combining class '2A' and '1A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51473\n",
       "2A      240\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['QScore'] = df.QScore.replace(['1A'], '2A')\n",
    "df.QScore.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33160</th>\n",
       "      <td>Kyrgyzstan</td>\n",
       "      <td>2016</td>\n",
       "      <td>113</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>2.290231e-01</td>\n",
       "      <td>1.540869e+00</td>\n",
       "      <td>0.105612578</td>\n",
       "      <td>1.368430e-01</td>\n",
       "      <td>4.267672e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.055024e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40653</th>\n",
       "      <td>Mauritania</td>\n",
       "      <td>2016</td>\n",
       "      <td>136</td>\n",
       "      <td>EFConsTotGHA</td>\n",
       "      <td>1.481184e+06</td>\n",
       "      <td>5.432897e+06</td>\n",
       "      <td>852452.4511</td>\n",
       "      <td>2.704210e+05</td>\n",
       "      <td>1.951359e+05</td>\n",
       "      <td>1.727426e+06</td>\n",
       "      <td>9.959516e+06</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39126</th>\n",
       "      <td>Mali</td>\n",
       "      <td>1993</td>\n",
       "      <td>133</td>\n",
       "      <td>EFProdPerCap</td>\n",
       "      <td>4.922205e-01</td>\n",
       "      <td>6.718759e-01</td>\n",
       "      <td>0.212944288</td>\n",
       "      <td>2.300339e-02</td>\n",
       "      <td>7.137923e-02</td>\n",
       "      <td>1.719190e-02</td>\n",
       "      <td>1.488615e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29675</th>\n",
       "      <td>Iraq</td>\n",
       "      <td>2016</td>\n",
       "      <td>103</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>4.605966e+06</td>\n",
       "      <td>6.261233e+05</td>\n",
       "      <td>1541760.681</td>\n",
       "      <td>7.977987e+04</td>\n",
       "      <td>1.046806e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.900435e+06</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69328</th>\n",
       "      <td>Congo, Democratic Republic of</td>\n",
       "      <td>1970</td>\n",
       "      <td>250</td>\n",
       "      <td>EFProdPerCap</td>\n",
       "      <td>3.008255e-01</td>\n",
       "      <td>2.181854e-02</td>\n",
       "      <td>0.53706</td>\n",
       "      <td>1.872915e-02</td>\n",
       "      <td>3.828945e-02</td>\n",
       "      <td>4.684737e-02</td>\n",
       "      <td>9.635702e-01</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6308</th>\n",
       "      <td>Bolivia</td>\n",
       "      <td>2016</td>\n",
       "      <td>19</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>4.825594e-01</td>\n",
       "      <td>1.662504e+00</td>\n",
       "      <td>0.171887765</td>\n",
       "      <td>1.046143e-02</td>\n",
       "      <td>6.755527e-02</td>\n",
       "      <td>7.894736e-01</td>\n",
       "      <td>3.184442e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20520</th>\n",
       "      <td>Finland</td>\n",
       "      <td>2016</td>\n",
       "      <td>67</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>5.116943e-01</td>\n",
       "      <td>8.317771e-02</td>\n",
       "      <td>4.458854285</td>\n",
       "      <td>1.977021e+00</td>\n",
       "      <td>9.190805e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.122655e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20523</th>\n",
       "      <td>Finland</td>\n",
       "      <td>2016</td>\n",
       "      <td>67</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>4.145792e+06</td>\n",
       "      <td>2.716590e+05</td>\n",
       "      <td>51643421.75</td>\n",
       "      <td>1.269716e+07</td>\n",
       "      <td>7.446472e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.950268e+07</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45343</th>\n",
       "      <td>Niger</td>\n",
       "      <td>1995</td>\n",
       "      <td>158</td>\n",
       "      <td>EFConsTotGHA</td>\n",
       "      <td>4.330783e+06</td>\n",
       "      <td>5.913366e+06</td>\n",
       "      <td>2940934.525</td>\n",
       "      <td>1.046740e+04</td>\n",
       "      <td>1.140384e+05</td>\n",
       "      <td>5.595589e+05</td>\n",
       "      <td>1.386915e+07</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28853</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>2004</td>\n",
       "      <td>101</td>\n",
       "      <td>EFConsTotGHA</td>\n",
       "      <td>9.455559e+07</td>\n",
       "      <td>4.746897e+06</td>\n",
       "      <td>52946760.84</td>\n",
       "      <td>3.723724e+07</td>\n",
       "      <td>1.174059e+07</td>\n",
       "      <td>1.178574e+08</td>\n",
       "      <td>3.190845e+08</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             country  year  country_code        record  \\\n",
       "33160                     Kyrgyzstan  2016           113    AreaPerCap   \n",
       "40653                     Mauritania  2016           136  EFConsTotGHA   \n",
       "39126                           Mali  1993           133  EFProdPerCap   \n",
       "29675                           Iraq  2016           103  BiocapTotGHA   \n",
       "69328  Congo, Democratic Republic of  1970           250  EFProdPerCap   \n",
       "6308                         Bolivia  2016            19  EFConsPerCap   \n",
       "20520                        Finland  2016            67    AreaPerCap   \n",
       "20523                        Finland  2016            67  BiocapTotGHA   \n",
       "45343                          Niger  1995           158  EFConsTotGHA   \n",
       "28853                      Indonesia  2004           101  EFConsTotGHA   \n",
       "\n",
       "          crop_land  grazing_land  forest_land  fishing_ground  built_up_land  \\\n",
       "33160  2.290231e-01  1.540869e+00  0.105612578    1.368430e-01   4.267672e-02   \n",
       "40653  1.481184e+06  5.432897e+06  852452.4511    2.704210e+05   1.951359e+05   \n",
       "39126  4.922205e-01  6.718759e-01  0.212944288    2.300339e-02   7.137923e-02   \n",
       "29675  4.605966e+06  6.261233e+05  1541760.681    7.977987e+04   1.046806e+06   \n",
       "69328  3.008255e-01  2.181854e-02      0.53706    1.872915e-02   3.828945e-02   \n",
       "6308   4.825594e-01  1.662504e+00  0.171887765    1.046143e-02   6.755527e-02   \n",
       "20520  5.116943e-01  8.317771e-02  4.458854285    1.977021e+00   9.190805e-02   \n",
       "20523  4.145792e+06  2.716590e+05  51643421.75    1.269716e+07   7.446472e+05   \n",
       "45343  4.330783e+06  5.913366e+06  2940934.525    1.046740e+04   1.140384e+05   \n",
       "28853  9.455559e+07  4.746897e+06  52946760.84    3.723724e+07   1.174059e+07   \n",
       "\n",
       "             carbon         total QScore  \n",
       "33160  0.000000e+00  2.055024e+00     2A  \n",
       "40653  1.727426e+06  9.959516e+06     2A  \n",
       "39126  1.719190e-02  1.488615e+00     3A  \n",
       "29675  0.000000e+00  7.900435e+06     2A  \n",
       "69328  4.684737e-02  9.635702e-01     3A  \n",
       "6308   7.894736e-01  3.184442e+00     3A  \n",
       "20520  0.000000e+00  7.122655e+00     2A  \n",
       "20523  0.000000e+00  6.950268e+07     2A  \n",
       "45343  5.595589e+05  1.386915e+07     3A  \n",
       "28853  1.178574e+08  3.190845e+08     3A  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2A = df[df.QScore=='2A']\n",
    "df_3A = df[df.QScore=='3A'].sample(350)\n",
    "data_df = df_2A.append(df_3A)\n",
    "data_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590, 12)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = shuffle(data_df)\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    350\n",
       "2A    240\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.QScore.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop(columns=['country_code', 'country', 'year'])\n",
    "X = data_df.drop(columns='QScore')\n",
    "y = data_df['QScore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    235\n",
       "2A    178\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still an imbalance in the class distribution. For this, we use SMOTE only on the training data to handle this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "X_train['record'] = encoder.fit_transform(X_train.record)\n",
    "X_test['record'] = encoder.fit_transform(X_test.record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=1)\n",
    "X_train_balanced, y_balanced = smote.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.786112e-02</td>\n",
       "      <td>1.029289e-02</td>\n",
       "      <td>1.735369e-02</td>\n",
       "      <td>1.715168e-03</td>\n",
       "      <td>8.648018e-02</td>\n",
       "      <td>0.009998</td>\n",
       "      <td>2.040938e-02</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.635984e-02</td>\n",
       "      <td>2.123552e-03</td>\n",
       "      <td>8.275276e-03</td>\n",
       "      <td>1.411935e-03</td>\n",
       "      <td>1.441415e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.827019e-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.175654e-02</td>\n",
       "      <td>1.158301e-01</td>\n",
       "      <td>2.504287e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.059039e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.614894e-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.631355e-02</td>\n",
       "      <td>2.363502e-02</td>\n",
       "      <td>1.216972e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.710721e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.952276e-03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.173502e-09</td>\n",
       "      <td>5.496204e-10</td>\n",
       "      <td>2.695632e-10</td>\n",
       "      <td>9.039378e-11</td>\n",
       "      <td>1.915074e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.537642e-10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crop_land  grazing_land   forest_land  fishing_ground  built_up_land  \\\n",
       "0  3.786112e-02  1.029289e-02  1.735369e-02    1.715168e-03   8.648018e-02   \n",
       "1  1.635984e-02  2.123552e-03  8.275276e-03    1.411935e-03   1.441415e-02   \n",
       "2  4.175654e-02  1.158301e-01  2.504287e-03    0.000000e+00   3.059039e-02   \n",
       "3  2.631355e-02  2.363502e-02  1.216972e-03    0.000000e+00   1.710721e-02   \n",
       "4  1.173502e-09  5.496204e-10  2.695632e-10    9.039378e-11   1.915074e-09   \n",
       "\n",
       "     carbon         total  record  \n",
       "0  0.009998  2.040938e-02       5  \n",
       "1  0.000000  5.827019e-03       1  \n",
       "2  0.000000  2.614894e-02       1  \n",
       "3  0.000000  7.952276e-03       3  \n",
       "4  0.000000  3.537642e-10       2  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "normalised_train_df = scaler.fit_transform(X_train_balanced.drop(columns=['record']))\n",
    "normalised_train_df = pd.DataFrame(normalised_train_df, columns=X_train_balanced.drop(columns=['record']).columns)\n",
    "normalised_train_df['record'] = X_train_balanced.record\n",
    "normalised_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>record</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.011359e-03</td>\n",
       "      <td>1.217512e-04</td>\n",
       "      <td>1.505777e-04</td>\n",
       "      <td>1.230732e-04</td>\n",
       "      <td>7.459960e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.911200e-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.314060e-03</td>\n",
       "      <td>4.147361e-04</td>\n",
       "      <td>5.087191e-04</td>\n",
       "      <td>7.624855e-05</td>\n",
       "      <td>5.392793e-03</td>\n",
       "      <td>1.120927e-02</td>\n",
       "      <td>6.872237e-04</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.765315e-02</td>\n",
       "      <td>1.535051e-03</td>\n",
       "      <td>1.270009e-02</td>\n",
       "      <td>1.275176e-02</td>\n",
       "      <td>1.417544e-01</td>\n",
       "      <td>6.275610e-01</td>\n",
       "      <td>2.736108e-02</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.291722e-10</td>\n",
       "      <td>5.867514e-10</td>\n",
       "      <td>3.142206e-12</td>\n",
       "      <td>4.058977e-11</td>\n",
       "      <td>9.098758e-10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.959085e-10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.467227e-10</td>\n",
       "      <td>5.549262e-12</td>\n",
       "      <td>6.663310e-11</td>\n",
       "      <td>7.212987e-12</td>\n",
       "      <td>5.536216e-10</td>\n",
       "      <td>4.220273e-10</td>\n",
       "      <td>3.853430e-11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crop_land  grazing_land   forest_land  fishing_ground  built_up_land  \\\n",
       "0  1.011359e-03  1.217512e-04  1.505777e-04    1.230732e-04   7.459960e-03   \n",
       "1  1.314060e-03  4.147361e-04  5.087191e-04    7.624855e-05   5.392793e-03   \n",
       "2  6.765315e-02  1.535051e-03  1.270009e-02    1.275176e-02   1.417544e-01   \n",
       "3  3.291722e-10  5.867514e-10  3.142206e-12    4.058977e-11   9.098758e-10   \n",
       "4  1.467227e-10  5.549262e-12  6.663310e-11    7.212987e-12   5.536216e-10   \n",
       "\n",
       "         carbon         total  record  \n",
       "0  0.000000e+00  2.911200e-04       1  \n",
       "1  1.120927e-02  6.872237e-04       7  \n",
       "2  6.275610e-01  2.736108e-02       5  \n",
       "3  0.000000e+00  1.959085e-10       2  \n",
       "4  4.220273e-10  3.853430e-11       6  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reset_index(drop=True)\n",
    "normalised_test_df = scaler.fit_transform(X_test.drop(columns=['record']))\n",
    "normalised_test_df = pd.DataFrame(normalised_test_df, columns=X_test.drop(columns=['record']).columns)\n",
    "normalised_test_df['record'] = X_test.record\n",
    "normalised_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(normalised_train_df, y_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Classification Performance {% fn 1 %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore cross validation techniques used by data scientist to avoid overfitting and enable generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation (CV) is a well known and trusted method applied to avoid overfitting and enable generalization. Although there are different techniques used in performing cross validation, the fundamental concept involves partitioning the dataset into a number of subsets, holding out a set for evaluation then training the model on the other sets. This gives a more reliable estimate of how the model performs across different training sets because it provides an average score across different training samples used. The only drawback with cross validation is that it takes more time and computational resources however, the gain obtained in having a better model is very well worth this cost. **K-Fold cross validation**, **Stratified K-Fold cross validation** and **Leave One Out Cross Validation (LOOCV)** are some cross validation techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54525074, 0.48843537, 0.52306785, 0.52078849, 0.51617647])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(log_reg, normalised_train_df, y_balanced, cv=5, scoring='f1_macro')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is called K-Fold because the data is split into K equal groups.  If $k = 5$ a 5-fold cross validation can be performed such that the data is split into $k_1$, $k_2$, $k_3$, $k_4$ and $k_5$. The model is trained on $k_2 - k_5$ and evaluated on $k_1$ then repeated $k$ times until every group is used to train and test the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](datasets/images/kfold.png \"kfold.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54.71698113207547,\n",
       " 54.99999999999999,\n",
       " 57.731958762886606,\n",
       " 62.18487394957983,\n",
       " 0.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "kf.split(normalised_train_df)\n",
    "f1_scores = []\n",
    "\n",
    "# run for every split\n",
    "for train_index, test_index in kf.split(normalised_train_df):\n",
    "    X_train_k, X_test_k = normalised_train_df.iloc[train_index], normalised_train_df.iloc[test_index]\n",
    "    y_train_k, y_test_k = y_balanced[train_index], y_balanced[test_index]\n",
    "    model = LogisticRegression().fit(X_train_k, y_train_k)\n",
    "    f1_scores.append(\n",
    "        f1_score(y_true=y_test_k, y_pred=model.predict(X_test_k), pos_label='2A')*100\n",
    "    )\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified K-Fold cross validation ensures that in every fold, there is an equal proportion of each target class to obtain a good representation of the data and avoid imbalance and biased results. For example, if there are two target classes $t_1$ and $t_2$ with equal distribution in the data, it is best to ensure that the folds also have the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5892857142857143,\n",
       " 0.6363636363636364,\n",
       " 0.6666666666666666,\n",
       " 0.5535714285714286,\n",
       " 0.6285714285714287]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "f1_scores_skf = []\n",
    "for train_index, test_index in skf.split(normalised_train_df, y_balanced):\n",
    "    X_train_skf, X_test_skf = np.array(normalised_train_df)[train_index], np.array(normalised_train_df)[test_index]\n",
    "    y_train_skf, y_test_skf = y_balanced[train_index], y_balanced[test_index]\n",
    "    model = LogisticRegression().fit(X_train_skf, y_train_skf)\n",
    "    f1_scores_skf.append(\n",
    "        f1_score(y_true=y_test_skf, y_pred=model.predict(X_test_skf), pos_label='2A')\n",
    "    )\n",
    "f1_scores_skf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out Cross Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, one instance is left out and used as the test set while the model is trained on $N-1$ data points where $N$ is the number of data points. This means that the number of instances and folds are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4829787234042553"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "scores_loo = cross_val_score(\n",
    "    LogisticRegression(), normalised_train_df, y_balanced, cv=loo, scoring='f1_macro'\n",
    ")\n",
    "average_score_loo = scores_loo.mean()\n",
    "average_score_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix, Precision-Recall, ROC curve and the F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, precision, recall, F1-score and many others are evaluation metrics used in measuring the performance of classification models. We will discuss these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is an $N$ x $N$ matrix that gives a summary of the correct and incorrect predicted classification results for the N target classes. The values in the diagonal of the matrix represent the number of correctly predicted classes while every other cell in the matrix indicates the misclassified classes. This means that the more predicted values that fall in the diagonal, the better the model. True positive, false positive, true negative and false negative are terms used when interpreting a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](datasets/images/confusion-matrix.png \"confusion-matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Positive (TP): \n",
    "This is a correct classification where the predicted value is the same as the actual value. Using the table above, this means that actual value was positive and the predicted value was also positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Negative (TN): \n",
    "The predicted value also matches the actual value. In this case, it is for the negative class. The actual value is negative and the predicted value is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positive (FP): \n",
    "Also called a Type I error, this is a misclassification such that the model predicted a positive class while the actual class is negative. Telling a man that he is pregnant is definitely a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Negative (FN): \n",
    "Also another misclassification where the predicted value is negative and the actual value is positive. Another example will be telling a pregnant woman that she is not pregnant. FN is known as a Type II error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2A', '3A', '3A', '2A', '2A'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction = log_reg.predict(normalised_test_df)\n",
    "new_prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 56,   6],\n",
       "       [102,  13]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_mat = confusion_matrix(y_true=y_test, y_pred=new_prediction, labels=['2A', '3A'])\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the ratio of the number of correctly predicted instances to the total number of instances. It is a commonly used metric suitable when the target classes are not imbalanced. A high accuracy does not necessarily mean that the model has high predicting power. Hence, depending on the task, it is important to not use only the accuracy metric because it does not provide enough information about the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.39\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_true=y_test, y_pred=new_prediction)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of correctly predicted instances of a class to the total number of items predicted by the model to be in that class is referred to as precision (known as Positive Predicted Value - PPV). This translates to the total percentage of the results obtained that are relevant. For the positive class, it is the ratio of true positives to the sum of true positives and false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.35\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_true=y_test, y_pred=new_prediction, pos_label='2A')\n",
    "print(f'precision: {precision:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Known as the sensitivity of the model, recall gives a percentage of total relevant results correctly predicted by the model. It is the ratio of the true positives to the actual number of positives (true positives and false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is also a trade-off between precision and recall. It is impossible to maximise both metrics simultaneously because an increase in recall decreases precision. Identify which metric is important based on your task and optimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.90\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_true=y_test, y_pred=new_prediction, pos_label='2A')\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is the harmonic mean of precision and recall that aims to have an optimal balance of both. The F1-Score is quite easy to use and can be focused on to maximize as opposed to maximizing precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F_1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.51\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_true=y_test, y_pred=new_prediction, pos_label='2A')\n",
    "print(f\"F1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristics (ROC) curve is a probability curve that measures the performance of a classification model at different set thresholds. Recall also known as the True Positive Rate (TPR) is plotted on the y-axis against the False Positive Rate (FPR) on the x-axis.\n",
    "\n",
    "The code examples above are not the optimal results that can be obtained with the model. Hyperparameter tuning can be performed to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore how to deal with more than two classes where an instance is classified into a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel and Multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classification deals with more than two classes where an instance is classified into a single class. For example, given a dataset with a set of features that describe the weather such that the classes are sunny, rainy and windy, a multiclass classification task will only give a single class as the result. In contrast, multilabel classification classifies an instance into a set of target labels. Articles and movies are examples where this can apply. An article can discuss a single topic but can also be about politics, religion, education and many more while movies are commonly tagged to multiple genres such as comedy, adventure, action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid and the Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function is quite similar to the sigmoid explained earlier. It is used for multiclass classification because it can obtain the probabilities for various classes such that the probabilities of each class sum to 1. This means that an increase in the probability of a class causes a decrease in the probability of at least one of the other classes. It can also be referred to as a generalization of logistic regression or the sigmoid function and can be used for multi-class classification while the sigmoid function is used in multi-label classification. The softmax function is popularly used in the output layers of neural networks. Although the sum of the outputs of the softmax must be 1, this is not the same for the sigmoid function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Methods and The Support Vector Machine {% fn 3 %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore Support Vector Machine (SVM), a supervised machine learning algorithm that is used to solve both classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and non-linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (SVM)  is a supervised machine learning algorithm that is used to solve both classification and regression tasks. In classification, the algorithm uses a line or hyperplane to separate classes by using data points close to the boundary (support vector)  for each class and a hyperplane that maximizes the distance between the classes. \n",
    "\n",
    "\n",
    "> Important: For clarity, a hyperplane is a line that linearly separates data points. Although there can be several hyperplanes between classes, the optimal hyperplane which has the maximum distance or margin between itself and the support vectors is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, data is not always linearly separable such that a straight line might not be able to adequately segregate classes. Although SVM is a linear classifier, it can be used to classify a non-linear dataset by transforming the dataset to a higher dimensional feature space where it can be linearly separable. This is done using the kernel trick such that a kernel function is applied on each data point to map to a higher dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees and CART algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree is a widely used non-parametric supervised machine learning approach that splits instances in a dataset based on different decision rules inferred from the features in the dataset. It is a tree-based algorithm with nodes that represent a specific attribute or decision rule such that for an instance, a question is asked at a node and possible answers to the question found on both edges. This is a sequential process that involves recursive partitioning of nodes for several features until the leaves for the tree provides the final output or class for that instance. Decision trees can also be used to solve regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3 - Iterative Dichotomiser 3, CART - Classification and Regression Trees, and C4.5 are some examples of decision tree algorithms. In this section, we only discuss the CART algorithm. The CART predictive model generates decision rules that have a binary tree representation such that each non-terminal node has two child nodes as opposed to some other tree-based methods that have more child nodes. It supports numerical target variables. At every node, the best split is chosen such that the splitting criterion is maximised. Gini impurity index is used as the splitting criterion in CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gini Impurity**: this is a measure of the chance that a randomly selected instance will be wrongly classified when selected. For different classes in a dataset, with $p(i)$ as the probability that the chosen instance belongs to class $i$, the gini impurity index for all classes $G$, can be calculated such that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini impurity index values range between 0 and 1 such that 0 translates to a pure classification where all instances belong to the same class while 1 means that there is a random distribution of the instances across different classes. To select the best split, the gini gain is calculated by taking a weighted sum of the gini impurity index then subtracting from the original impurity. Higher gini gain leads to better splits simply put, the lower the gini impurity, the better the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting in Decision Trees, Early Stopping and Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive partitioning of nodes until the final subsets are obtained in decision trees makes it prone to overfitting. The deeper the tree, the higher the chances of the overfitting. This can be prevented using a stopping criterion such as early stopping and pruning. Early stopping or pre-pruning involves stopping the tree-building process before the tree becomes too complex and the training data is perfectly classified. An early stopping condition like the maximum depth can be set to avoid deep trees such that the tree stops growing after reaching the set maximum depth for the tree. Another early stopping criterion that can be used is the classification error. At every splitting stage, the error is checked. If there is no significant decrease in the error, there is no need to make the tree more complex. When there are fewer data points than a set threshold value, early stopping can also take place. Early stopping may also produce underfit models if it stops too early. Post-pruning, on the other hand, allows the tree to be fully built before simplifying by removing sections of the tree at different levels by calculating the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2A', '2A', '3A', '3A', '3A'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(normalised_train_df, y_balanced)\n",
    "dtc_pred = dtc.predict(normalised_test_df)\n",
    "dtc_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore how to combine several classifiers to obtain an optimal model with better performance as opposed to just a single classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond decision trees and ensemble classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembling in machine learning involves the combination of several classifiers to obtain an optimal model with better performance as opposed to just a single classifier. These classifiers can be of different algorithms and hyperparameters. Bagging, boosting, stacking and blending are methods classifiers can be combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap Aggregation or Bagging is a parallel ensembling technique that randomly bootstraps or samples the dataset with replacement to create subsets from the original. Multiple models are then trained using these subsets and the predicted results from these models aggregated to return final predictions. Bagging results in a final model that has less variance than its base classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When bagging is applied to decision trees, it results in random forests which is a supervised learning algorithm that has a large number of decision trees. For an instance in the dataset, each tree returns a prediction for the class the instance belongs to then, the class with the most votes becomes the final class for that instance. In random forests, it is assumed that a group of uncorrelated trees will do better than an individual tree. While some of the trees might be wrong in their predictions, many others will be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting: AdaBoost, Gradient Boosting and XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a sequential process where every phase attempts to correct the errors made by the previous model. The main principle is to fit multiple weak learners which are slightly better than just random guessing. In contrast to bagging, boosting attempts to reduce both variance and bias. AdaBoost, Gradient Boosting and XGBoost are examples of boosting algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost: \n",
    "\n",
    "Adaptive Boosting is the first boosting algorithm. It is a very popular method for boosting that can be used on any classifier to present a more accurate model and improve its performance.  It can be described with the following steps: create a subset from the entire dataset, assign equal weights to the data points, create a base model using this subset, predict using this model, calculate errors from the predicted results, assign higher weights to misclassified instances to increase their chances of being selected, create another model that tries to correct these mistakes and make new predictions then repeat until the maximum number of models specified are created. The final model is the weighted average of all the weak learners created. AdaBoost is very sensitive to noisy data and outliers so it is important to remove these when using AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting: \n",
    "\n",
    "This is another boosting algorithm that improves model performance where each model in the ensemble minimizes a loss function using gradient descent. The loss function which is used to obtain an estimate of how the model is performing, a weak learner - a model only slightly better than random guessing typically decision stumps (a decision tree with a single split - one level) and an additive model that combines the weak learners to make the final model are three important components in gradient boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost: \n",
    "Extreme Gradient Boosting is a supervised learning algorithm that implements gradient boosting by building trees parallely while applying regularization. It is well known for its scalability and fast execution. XGBoost can automatically identify missing values in data and it builds very deep trees before pruning for optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading Resources {% fn 4 %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "{{'The codes can also be found [here: Linear Classification and Logistic Regression](https://gist.github.com/HamoyeHQ/94d52ad113d1eac80d073a4affb0a490)' | fndetail: 1}}\n",
    "{{'The codes can also be found [here: Measuring Classification Performance](https://gist.github.com/HamoyeHQ/bf8f7062e2acbaa48dc94993e8487b3d)' | fndetail: 2}}\n",
    "{{'The codes can also be found [here: Tree-Based Methods and The Support Vector Machine](https://gist.github.com/HamoyeHQ/fb9265ee0d668480918466583d143f2f)' | fndetail: 3}}\n",
    "{{'Additional Reading List and Links\n",
    "[Ensemble Learning: Bagging and Boosting](https://becominghuman.ai/ensemble-learning-bagging-and-boosting-d20f38be9b1e)\n",
    "[Feature Engineering by Wale Akinfaderin.](https://www.youtube.com/watch?v=ZQ5wF7z01I0)\n",
    "[Scikit-Learn Classification.](https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/)\n",
    "[Gentle Introduction to XGBoost.](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)\n",
    "[Learning from Imbalanced Class.](https://www.jeremyjordan.me/imbalanced-data/)\n",
    "[Hands-on Machine Learning - NUMBER ONE GUIDE'](https://www.lpsm.paris/pageperso/has/source/Hand-on-ML.pdf) | fndetail: 4}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
